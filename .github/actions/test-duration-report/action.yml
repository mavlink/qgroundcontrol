name: Test Duration Report
description: Analyze JUnit test durations, summarize slow tests, and warn on regressions.

inputs:
  junit-path:
    description: Path to JUnit XML report generated by CTest
    required: true
  baseline-path:
    description: Path to JSON baseline file for regression detection
    required: false
    default: .github/test-duration-baseline.json
  report-json-path:
    description: Where to write computed timing report JSON
    required: false
    default: test-duration-report.json
  top-n:
    description: Number of slowest tests to include in summary
    required: false
    default: '20'
  slow-threshold-seconds:
    description: Warn when a test exceeds this absolute duration
    required: false
    default: '60'
  regression-factor:
    description: Warn when current_time >= baseline_time * factor
    required: false
    default: '1.5'
  min-delta-seconds:
    description: Minimum absolute increase required to flag a regression
    required: false
    default: '5'
  fail-on-regression:
    description: Fail the step when regressions are detected
    required: false
    default: 'false'

outputs:
  regression-count:
    description: Number of detected timing regressions
    value: ${{ steps.analyze.outputs.regression_count }}
  slow-count:
    description: Number of tests over the absolute slow threshold
    value: ${{ steps.analyze.outputs.slow_count }}

runs:
  using: composite
  steps:
    - name: Analyze test durations
      id: analyze
      shell: bash
      run: |
        python3 - <<'PY'
        import json
        import os
        import sys
        import xml.etree.ElementTree as ET
        from pathlib import Path

        junit_path = Path(r"${{ inputs.junit-path }}")
        baseline_path = Path(r"${{ inputs.baseline-path }}")
        report_json_path = Path(r"${{ inputs.report-json-path }}")
        top_n = int(r"${{ inputs.top-n }}")
        slow_threshold = float(r"${{ inputs.slow-threshold-seconds }}")
        regression_factor = float(r"${{ inputs.regression-factor }}")
        min_delta = float(r"${{ inputs.min-delta-seconds }}")
        fail_on_regression = r"${{ inputs.fail-on-regression }}".lower() == "true"

        step_summary = Path(os.environ["GITHUB_STEP_SUMMARY"])
        github_output = Path(os.environ["GITHUB_OUTPUT"])

        def test_key(elem: ET.Element) -> str:
            classname = elem.attrib.get("classname", "").strip()
            name = elem.attrib.get("name", "").strip()
            if classname and name:
                return f"{classname}::{name}"
            return name or classname or "<unnamed>"

        def parse_time(value: str) -> float:
            try:
                return float(value)
            except Exception:
                return 0.0

        if not junit_path.exists():
            msg = f"JUnit report not found at {junit_path}"
            print(f"::warning::{msg}")
            with step_summary.open("a", encoding="utf-8") as f:
                f.write("## Test Duration Report\n\n")
                f.write(f"- WARNING: {msg}\n")
            with github_output.open("a", encoding="utf-8") as f:
                f.write("regression_count=0\n")
                f.write("slow_count=0\n")
            sys.exit(0)

        tree = ET.parse(junit_path)
        root = tree.getroot()
        cases = []
        for testcase in root.iter("testcase"):
            key = test_key(testcase)
            secs = parse_time(testcase.attrib.get("time", "0"))
            cases.append((key, secs))

        total_seconds = sum(secs for _, secs in cases)
        slowest = sorted(cases, key=lambda x: x[1], reverse=True)
        slow_over_threshold = [(k, s) for k, s in slowest if s >= slow_threshold]

        baseline = {}
        baseline_loaded = False
        if baseline_path.exists():
            try:
                payload = json.loads(baseline_path.read_text(encoding="utf-8"))
                if isinstance(payload, dict) and "tests" in payload and isinstance(payload["tests"], dict):
                    raw = payload["tests"]
                    for key, value in raw.items():
                        if isinstance(value, dict):
                            baseline[key] = float(value.get("seconds", 0.0))
                        else:
                            baseline[key] = float(value)
                    baseline_loaded = True
                elif isinstance(payload, dict):
                    for key, value in payload.items():
                        if isinstance(value, (int, float)):
                            baseline[key] = float(value)
                    baseline_loaded = True
            except Exception as e:
                print(f"::warning::Failed to parse baseline file {baseline_path}: {e}")

        regressions = []
        if baseline:
            for key, current in cases:
                previous = baseline.get(key)
                if previous is None or previous <= 0:
                    continue
                if current >= previous * regression_factor and (current - previous) >= min_delta:
                    regressions.append((key, previous, current))

        report = {
            "junit_path": str(junit_path),
            "total_tests": len(cases),
            "total_seconds": total_seconds,
            "slow_threshold_seconds": slow_threshold,
            "slow_tests": [{"test": k, "seconds": s} for k, s in slow_over_threshold],
            "regression_factor": regression_factor,
            "min_delta_seconds": min_delta,
            "regressions": [
                {"test": k, "baseline_seconds": b, "current_seconds": c, "delta_seconds": c - b}
                for k, b, c in regressions
            ],
        }
        report_json_path.parent.mkdir(parents=True, exist_ok=True)
        report_json_path.write_text(json.dumps(report, indent=2, sort_keys=True), encoding="utf-8")

        with step_summary.open("a", encoding="utf-8") as f:
            f.write("## Test Duration Report\n\n")
            f.write(f"- JUnit: `{junit_path}`\n")
            f.write(f"- Total tests: **{len(cases)}**\n")
            f.write(f"- Total time: **{total_seconds:.1f}s**\n")
            f.write(f"- Slow threshold: **{slow_threshold:.1f}s**\n")
            if baseline_loaded:
                f.write(f"- Baseline file: `{baseline_path}` ({len(baseline)} entries)\n")
            else:
                f.write(f"- Baseline file: `{baseline_path}` (not available)\n")
            f.write("\n")

            if slowest:
                f.write(f"### Top {min(top_n, len(slowest))} Slowest Tests\n\n")
                f.write("| Test | Seconds |\n")
                f.write("|---|---:|\n")
                for key, secs in slowest[:top_n]:
                    f.write(f"| `{key}` | {secs:.3f} |\n")
                f.write("\n")

            if regressions:
                f.write("### Regressions vs Baseline\n\n")
                f.write("| Test | Baseline (s) | Current (s) | Delta (s) |\n")
                f.write("|---|---:|---:|---:|\n")
                for key, base, cur in sorted(regressions, key=lambda x: x[2] - x[1], reverse=True)[:top_n]:
                    f.write(f"| `{key}` | {base:.3f} | {cur:.3f} | {cur - base:.3f} |\n")
                f.write("\n")
            else:
                f.write("No baseline regressions detected.\n\n")

        for key, secs in slow_over_threshold[:top_n]:
            print(f"::warning::Slow test (>={slow_threshold:.1f}s): {key} took {secs:.3f}s")

        for key, base, cur in sorted(regressions, key=lambda x: x[2] - x[1], reverse=True)[:top_n]:
            print(
                f"::warning::Timing regression: {key} baseline={base:.3f}s current={cur:.3f}s "
                f"(+{(cur - base):.3f}s, x{(cur / base):.2f})"
            )

        with github_output.open("a", encoding="utf-8") as f:
            f.write(f"regression_count={len(regressions)}\n")
            f.write(f"slow_count={len(slow_over_threshold)}\n")

        if fail_on_regression and regressions:
            sys.exit(1)
        PY
